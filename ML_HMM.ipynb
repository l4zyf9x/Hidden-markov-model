{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-HMM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l4zyf9x/Hidden-markov-model/blob/master/ML_HMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "I4A0IhqnvEnF",
        "colab_type": "code",
        "outputId": "b39d5f4e-4b9e-4180-d4d9-b3aaa56f2541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/l4zyf9x/Hiden-markov-model.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Hiden-markov-model'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/30)   \u001b[K\rremote: Counting objects:   6% (2/30)   \u001b[K\rremote: Counting objects:  10% (3/30)   \u001b[K\rremote: Counting objects:  13% (4/30)   \u001b[K\rremote: Counting objects:  16% (5/30)   \u001b[K\rremote: Counting objects:  20% (6/30)   \u001b[K\rremote: Counting objects:  23% (7/30)   \u001b[K\rremote: Counting objects:  26% (8/30)   \u001b[K\rremote: Counting objects:  30% (9/30)   \u001b[K\rremote: Counting objects:  33% (10/30)   \u001b[K\rremote: Counting objects:  36% (11/30)   \u001b[K\rremote: Counting objects:  40% (12/30)   \u001b[K\rremote: Counting objects:  43% (13/30)   \u001b[K\rremote: Counting objects:  46% (14/30)   \u001b[K\rremote: Counting objects:  50% (15/30)   \u001b[K\rremote: Counting objects:  53% (16/30)   \u001b[K\rremote: Counting objects:  56% (17/30)   \u001b[K\rremote: Counting objects:  60% (18/30)   \u001b[K\rremote: Counting objects:  63% (19/30)   \u001b[K\rremote: Counting objects:  66% (20/30)   \u001b[K\rremote: Counting objects:  70% (21/30)   \u001b[K\rremote: Counting objects:  73% (22/30)   \u001b[K\rremote: Counting objects:  76% (23/30)   \u001b[K\rremote: Counting objects:  80% (24/30)   \u001b[K\rremote: Counting objects:  83% (25/30)   \u001b[K\rremote: Counting objects:  86% (26/30)   \u001b[K\rremote: Counting objects:  90% (27/30)   \u001b[K\rremote: Counting objects:  93% (28/30)   \u001b[K\rremote: Counting objects:  96% (29/30)   \u001b[K\rremote: Counting objects: 100% (30/30)   \u001b[K\rremote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/17)   \u001b[K\rremote: Compressing objects:  11% (2/17)   \u001b[K\rremote: Compressing objects:  17% (3/17)   \u001b[K\rremote: Compressing objects:  23% (4/17)   \u001b[K\rremote: Compressing objects:  29% (5/17)   \u001b[K\rremote: Compressing objects:  35% (6/17)   \u001b[K\rremote: Compressing objects:  41% (7/17)   \u001b[K\rremote: Compressing objects:  47% (8/17)   \u001b[K\rremote: Compressing objects:  52% (9/17)   \u001b[K\rremote: Compressing objects:  58% (10/17)   \u001b[K\rremote: Compressing objects:  64% (11/17)   \u001b[K\rremote: Compressing objects:  70% (12/17)   \u001b[K\rremote: Compressing objects:  76% (13/17)   \u001b[K\rremote: Compressing objects:  82% (14/17)   \u001b[K\rremote: Compressing objects:  88% (15/17)   \u001b[K\rremote: Compressing objects:  94% (16/17)   \u001b[K\rremote: Compressing objects: 100% (17/17)   \u001b[K\rremote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 30 (delta 10), reused 30 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5PLuBDkRxqs_",
        "colab_type": "code",
        "outputId": "e29fa050-b6ba-445a-f71d-3b622bb8ada8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/Hiden-markov-model\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Hiden-markov-model\n",
            "corpus\thmm.py\tREADME.md  sumary.txt  util.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-RDA9M-Vyw5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def expand_matrix(matrix, length, value=0):\n",
        "    \"\"\" \n",
        "    Return a array with new shape which has all element increasing `length`. And\n",
        "    fill new variable with `value`\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> a=np.zeros((3,4,3))\n",
        "\n",
        "    >>> expand_matrix(a, 2)\n",
        "\n",
        "    >>> a.shape # (5,6,5) \n",
        "    \"\"\"\n",
        "    # Number dimension of matrix\n",
        "    d = len(matrix.shape)\n",
        "    # Expand one by one dimesion\n",
        "    for i in range(d):\n",
        "        matrix = expand_one_dimesion(matrix, length, i, value)\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def expand_one_dimesion(matrix, length, dimension, value=0):\n",
        "    \"\"\" \n",
        "    Return a array with new shape which increasing `length` on specify `dimension`. And\n",
        "    fill new variable with `value`\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> x = np.zeros((3, 4, 5))\n",
        "\n",
        "    >>> x = expand_one_dimesion(x, 3, 1)\n",
        "\n",
        "\n",
        "    >>> x.shape # (3, 7, 5) \n",
        "    \"\"\"\n",
        "    d = len(matrix.shape)\n",
        "\n",
        "    if dimension > d or dimension < 0:\n",
        "        raise ValueError('dimesion is invalid')\n",
        "    # Expand one by one dimesion\n",
        "    extended_shape = tuple([length if(idx == dimension) else old_length for idx, old_length in enumerate(matrix.shape)])\n",
        "    # Create new extend\n",
        "    extended_element = np.ones(extended_shape) * value\n",
        "    return np.concatenate((matrix, extended_element), axis=dimension)\n",
        "\n",
        "class HMM:\n",
        "    def __init__(self, ngram, delta=0.1):\n",
        "        self.ngram = ngram\n",
        "        \"\"\"\n",
        "        tag maps to normalize some tags in data set\n",
        "        \"\"\"\n",
        "        self.tags_map = {\n",
        "            \"Ab\": \"A\",\n",
        "            \"B\": \"FW\",\n",
        "            \"Fw\": \"FW\",\n",
        "            \"Nb\": \"FW\",\n",
        "            \"Ne\": \"Nc\",\n",
        "            \"Ni\": \"Np\",\n",
        "            \"NNP\": \"Np\",\n",
        "            \"Ns\": \"Nc\",\n",
        "            \"S\": \"Z\",\n",
        "            \"Vb\": \"V\",\n",
        "            \"Y\": \"Np\"\n",
        "        }\n",
        "        self.delta = delta\n",
        "        self.tags = {'Q0': 0}\n",
        "        self.vocab = {}\n",
        "        transition_shape = tuple([1 for i in range(ngram)])\n",
        "        self.Q = np.zeros(transition_shape)\n",
        "        self.E = np.zeros(tuple([1, 0]))\n",
        "        self.export_file_name = 'result/{}{}{}'\n",
        "        self.lamda = 0.1\n",
        "\n",
        "    def normalizeTag(self, tag):\n",
        "        if tag in self.tags_map:\n",
        "            return self.tags_map[tag]\n",
        "        else:\n",
        "            return tag\n",
        "\n",
        "    def initialize_matrix(self, corpus):\n",
        "        for idx, sentence in enumerate(corpus):\n",
        "            pre_states = deque([self.tags['Q0']\n",
        "                                for i in range(self.ngram - 1)])\n",
        "            for token in sentence.tokens:\n",
        "                word = token.getWord()\n",
        "                tag = token.getTag()\n",
        "                if word not in self.vocab:\n",
        "                    self.vocab[word] = len(self.vocab)\n",
        "                    self.E = util.expand_one_dimesion(self.E, 1, 1)\n",
        "                if tag not in self.tags:\n",
        "                    self.tags[tag] = len(self.tags)\n",
        "                    self.Q = util.expand_matrix(self.Q, 1)\n",
        "                    self.E = util.expand_one_dimesion(self.E, 1, 0)\n",
        "\n",
        "                self.E[self.tags[tag], self.vocab[word]] += 1\n",
        "                pre_states.append(self.tags[tag])\n",
        "                self.Q[tuple(pre_states)] += 1\n",
        "                pre_states.popleft()\n",
        "            if (idx+1) % 500 == 0:\n",
        "                print('> Read sentence {}'.format(idx))\n",
        "        transition_sum = np.sum(self.Q, axis=-1, keepdims=True)\n",
        "        transition_sum[transition_sum == 0] = -1\n",
        "        emission_sum = np.sum(self.E, axis=-1, keepdims=True)\n",
        "        self.Q = (self.Q + self.delta) / \\\n",
        "            (transition_sum + self.delta*len(self.vocab))\n",
        "        self.E = (self.E + self.delta) / \\\n",
        "            (emission_sum + self.delta*len(self.vocab))\n",
        "        print('> shape Q: {}, shape E: {}'.format(self.Q.shape, self.E.shape))\n",
        "\n",
        "    def saveModel(self, name='model'):\n",
        "        # time = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M')\n",
        "        time = ''\n",
        "#         basePath = os.path.dirname(os.path.abspath(__file__))\n",
        "        basePath = '/content/Hiden-markov-model'\n",
        "        resultPath = basePath + '/result'\n",
        "        if not os.path.exists(resultPath):\n",
        "            os.makedirs(resultPath)\n",
        "            print('> Create new folder {}'.format(resultPath))\n",
        "        with open(self.export_file_name.format('vocab', time, '.json'), 'w') as fp:\n",
        "            json.dump(self.vocab, fp)\n",
        "        with open(self.export_file_name.format('tags', time, '.json'), 'w') as fp:\n",
        "            json.dump(self.tags, fp)\n",
        "\n",
        "        np.save(self.export_file_name.format('transition-matrix', time, '.npy'),\n",
        "                self.Q)\n",
        "        np.savetxt(self.export_file_name.format('emission-matrix', time, '.txt'),\n",
        "                   self.E, delimiter=',')\n",
        "\n",
        "    def loadModel(self):\n",
        "        time = ''\n",
        "        with open(self.export_file_name.format('vocab', time, '.json'), 'r') as fp:\n",
        "            print('> Load vocabulary')\n",
        "            self.vocab = json.load(fp)\n",
        "        with open(self.export_file_name.format('tags', time, '.json'), 'r') as fp:\n",
        "            self.tags = json.load(fp)\n",
        "            print('> Load tags')\n",
        "\n",
        "        self.Q = np.load(self.export_file_name.format(\n",
        "            'transition-matrix', time, '.npy'))\n",
        "        print('> Load transition matrix. Shape: {}'.format(self.Q.shape))\n",
        "        self.E = np.loadtxt(self.export_file_name.format(\n",
        "            'emission-matrix', time, '.txt'), delimiter=',')\n",
        "        print('> Load emission matrix. Shape: {}'.format(self.E.shape))\n",
        "\n",
        "    def get_tag(self, search_index):\n",
        "        for tag in self.tags:    # for name, age in list.items():  (for Python 3.x)\n",
        "            if self.tags[tag] == search_index:\n",
        "                return tag\n",
        "        return None\n",
        "\n",
        "    def loadCorpus(self, fileName):\n",
        "        print('> Start read file: {}'.format(fileName))\n",
        "        corpus = []\n",
        "        with open(fileName) as fp:\n",
        "            newSetence = Sentence()\n",
        "            for line in fp:\n",
        "                line = line.strip()\n",
        "                line = line\n",
        "                parts = line.rpartition('\\t')\n",
        "                word = parts[0].lower()\n",
        "                tag = self.normalizeTag(parts[-1])\n",
        "                if(word == ''):\n",
        "                    corpus.append(newSetence)\n",
        "                    newSetence = Sentence()\n",
        "                else:\n",
        "                    newSetence.addToken(Token(word, tag))\n",
        "            corpus.append(newSetence)\n",
        "        print('> End read file: {}'.format(fileName))\n",
        "        return corpus\n",
        "\n",
        "    def decode(self, sentence):\n",
        "        if len(sentence) > 0:\n",
        "            # V(tags,sentence length) denote for viterbi matrix\n",
        "            V = np.zeros((len(self.tags), len(sentence)), dtype=np.float64)\n",
        "            # backtrack(tags,sentence length)\n",
        "            backtrack = np.zeros(\n",
        "                (len(self.tags), len(sentence)), dtype=np.int16)\n",
        "\n",
        "            if(sentence[0] not in self.vocab):\n",
        "                # Add new word into vocabulary and assign new word emission = 1/len(vocab)\n",
        "                self.vocab[sentence[0]] = len(self.vocab)\n",
        "                self.E = util.expand_one_dimesion(\n",
        "                    self.E, 1, 1, 1/(len(self.vocab)))\n",
        "            # Initialize vitebi 0\n",
        "            # V[i,0] = Q[Si|Q0..Q0] * E(W|Si)\n",
        "            V[:, 0] = self.Q[tuple([self.tags['Q0'] for i in range(self.ngram - 1)])\n",
        "                             ] * self.E[:, self.vocab[sentence[0]]]\n",
        "            # Given P(Si|Sj...Sk) in transition matrix. Denote Sj...Sk as previous state\n",
        "            pre_states = np.zeros(\n",
        "                (len(self.tags), self.ngram-1), dtype=np.int16)\n",
        "            pre_states[:, -1] = range(len(self.tags))\n",
        "            # Start from word at position 2.\n",
        "            # Value index indicate previous word's viterbi value\n",
        "            for index, word in enumerate(sentence[1:]):\n",
        "                if word not in self.vocab:\n",
        "                    # Add new word into vocabulary and assign new word emission = 1/len(vocab)\n",
        "                    self.vocab[word] = len(self.vocab)\n",
        "                    self.E = util.expand_one_dimesion(\n",
        "                        self.E, 1, 1, 1/(len(self.vocab)))\n",
        "                # V_candidate(states, states) denote for all likelihood probability for all state at current word\n",
        "                # V_candidate[i,j] = V_before[j] * P[Si|Sj...Sk] * P[W|Si]\n",
        "                V_candidate = V[:, index] * \\\n",
        "                    np.array([self.Q[tuple(prev)] for prev in pre_states]).T * \\\n",
        "                    self.E[:, self.vocab[word]].reshape((-1, 1))\n",
        "                # Find max each row to update viterbi matrix for current word\n",
        "                # Update back track matrix\n",
        "                V[:, index+1] = np.amax(V_candidate, axis=1)\n",
        "                backtrack[:, index+1] = np.argmax(V_candidate, axis=1)\n",
        "                # Update previous state for next word\n",
        "                temp_states = np.copy(pre_states)\n",
        "                for i, prev in enumerate(backtrack[:, index+1]):\n",
        "                    pre_states[i, :-1] = temp_states[prev, 1:]\n",
        "                    pre_states[i, -1] = i\n",
        "            tag_indexs = []\n",
        "            tag_indexs.append(np.argmax(V[:, -1]))\n",
        "            for i, col in enumerate(backtrack[:, ::-1].T):\n",
        "                tag_indexs.append(col[tag_indexs[-1]])\n",
        "            rs = [self.get_tag(i) for i in tag_indexs]\n",
        "            rs.reverse()\n",
        "            return rs[1:]\n",
        "\n",
        "    def evaluate(self, corpus):\n",
        "        correct_num = 0\n",
        "        for index, sentence in enumerate(corpus):\n",
        "            predict_tag = self.decode(sentence.getWords())\n",
        "            # print('> Sentence {}:'.format(index))\n",
        "            # print('> predict_tag: {}'.format(predict_tag))\n",
        "            label_tag = sentence.getTags()\n",
        "            count = 0\n",
        "            for idx, tag in enumerate(predict_tag):\n",
        "                if (tag == label_tag[idx]):\n",
        "                    count += 1\n",
        "            correct_num+= count/len(predict_tag)\n",
        "        return correct_num/len(corpus)\n",
        "\n",
        "\n",
        "class Sentence():\n",
        "    def __init__(self):\n",
        "        self.tokens = []\n",
        "\n",
        "    def addToken(self, token):\n",
        "        self.tokens.append(token)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def getWords(self):\n",
        "        return [token.getWord() for token in self.tokens]\n",
        "\n",
        "    def getTags(self):\n",
        "        return [token.getTag() for token in self.tokens]\n",
        "\n",
        "\n",
        "class Token():\n",
        "    def __init__(self, word, tag):\n",
        "        self.word = word\n",
        "        self.tag = tag\n",
        "\n",
        "    def getWord(self):\n",
        "        return self.word\n",
        "\n",
        "    def getTag(self):\n",
        "        return self.tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IqIWJAWpzgOK",
        "colab_type": "code",
        "outputId": "7eea786d-3b33-4bde-89f0-2d17eb147145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "cell_type": "code",
      "source": [
        "trainFile = 'corpus/train/train.txt'\n",
        "testFile = 'corpus/vlsp2016/test.txt'\n",
        "model = HMM(3)\n",
        "corpus = model.loadCorpus(trainFile)\n",
        "print('Train corpus length: ', len(corpus))\n",
        "test_corpus = model.loadCorpus(testFile)\n",
        "print('Train corpus length: ', len(test_corpus))\n",
        "model.initialize_matrix(corpus)\n",
        "model.saveModel()\n",
        "# model.loadModel()\n",
        "print('Accuracy on test set : ', model.evaluate(test_corpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Start read file: corpus/train/train.txt\n",
            "> End read file: corpus/train/train.txt\n",
            "Train corpus length:  22144\n",
            "> Start read file: corpus/vlsp2016/test.txt\n",
            "> End read file: corpus/vlsp2016/test.txt\n",
            "Train corpus length:  2831\n",
            "> Read sentence 499\n",
            "> Read sentence 999\n",
            "> Read sentence 1499\n",
            "> Read sentence 1999\n",
            "> Read sentence 2499\n",
            "> Read sentence 2999\n",
            "> Read sentence 3499\n",
            "> Read sentence 3999\n",
            "> Read sentence 4499\n",
            "> Read sentence 4999\n",
            "> Read sentence 5499\n",
            "> Read sentence 5999\n",
            "> Read sentence 6499\n",
            "> Read sentence 6999\n",
            "> Read sentence 7499\n",
            "> Read sentence 7999\n",
            "> Read sentence 8499\n",
            "> Read sentence 8999\n",
            "> Read sentence 9499\n",
            "> Read sentence 9999\n",
            "> Read sentence 10499\n",
            "> Read sentence 10999\n",
            "> Read sentence 11499\n",
            "> Read sentence 11999\n",
            "> Read sentence 12499\n",
            "> Read sentence 12999\n",
            "> Read sentence 13499\n",
            "> Read sentence 13999\n",
            "> Read sentence 14499\n",
            "> Read sentence 14999\n",
            "> Read sentence 15499\n",
            "> Read sentence 15999\n",
            "> Read sentence 16499\n",
            "> Read sentence 16999\n",
            "> Read sentence 17499\n",
            "> Read sentence 17999\n",
            "> Read sentence 18499\n",
            "> Read sentence 18999\n",
            "> Read sentence 19499\n",
            "> Read sentence 19999\n",
            "> Read sentence 20499\n",
            "> Read sentence 20999\n",
            "> Read sentence 21499\n",
            "> Read sentence 21999\n",
            "> shape Q: (21, 21, 21), shape E: (21, 17014)\n",
            "Accuracy on test set :  0.8825643679256882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fp4xHAyWzjc_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}